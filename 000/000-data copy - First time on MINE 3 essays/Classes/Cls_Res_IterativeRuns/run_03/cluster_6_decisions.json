{
  "cluster_label": 6,
  "cluster_classes": [
    {
      "candidate_id": "ClsC_890d54c4",
      "class_label": "AI Decision Explainability",
      "class_description": "Concepts related to AI systems’ decisions and the human-understandable explanations of how those decisions are produced, supporting transparency and accountability.",
      "class_type_hint": "AI Governance Concept",
      "class_group": "TBD",
      "confidence": 0.86,
      "evidence_excerpt": "Human-understandable explanations of how AI systems make decisions and execute internal processes; AI system decisions as outputs or choices.",
      "member_ids": [
        "Can_c1751366",
        "En_560ba8c4"
      ],
      "members": [
        {
          "id": "Can_c1751366",
          "entity_name": "explanations for AI decisions and processes",
          "entity_description": "Human-understandable justifications and descriptions of how AI systems make decisions and execute internal processes, used to improve transparency, trust, and accountability.",
          "entity_type_hint": "MitigationAction"
        },
        {
          "id": "En_560ba8c4",
          "entity_name": "AI system decisions",
          "entity_description": "Outputs or choices produced by AI systems in response to inputs or situations.",
          "entity_type_hint": "Event"
        }
      ],
      "remarks": []
    },
    {
      "candidate_id": "ClsC_75828e8c",
      "class_label": "AI System Transparency",
      "class_description": "Concepts describing the transparency of AI systems and its role in shaping stakeholder perceptions, including both the presence and absence of transparency and its impact on trust, acceptance, and adoption.",
      "class_type_hint": "Socio-Technical Concept",
      "class_group": "TBD",
      "confidence": 0.93,
      "evidence_excerpt": "Degree to which AI systems’ processes are understandable vs. condition where decision-making is not visible.",
      "member_ids": [
        "Can_36886a98",
        "En_e13e3f0a"
      ],
      "members": [
        {
          "id": "Can_36886a98",
          "entity_name": "transparency in AI systems",
          "entity_description": "The degree to which AI systems’ processes and decision logic are open, understandable, and explainable to stakeholders, used both as a general property and as a design practice to enhance trust and accountability.",
          "entity_type_hint": "Concept"
        },
        {
          "id": "En_e13e3f0a",
          "entity_name": "lack of transparency",
          "entity_description": "Condition where AI systems’ decision-making processes are not visible or understandable to stakeholders.",
          "entity_type_hint": "Condition"
        }
      ],
      "remarks": []
    },
    {
      "candidate_id": "ClsC_2437d804",
      "class_label": "Opaque AI Systems",
      "class_description": "AI systems whose internal decision-making processes are not readily interpretable or observable, raising challenges for transparency, accountability, and oversight.",
      "class_type_hint": "System Characteristic",
      "class_group": "TBD",
      "confidence": 0.9,
      "evidence_excerpt": "AI systems become more complex and opaque, making it challenging to understand how they arrive at their decisions.",
      "member_ids": [
        "En_b71727ac",
        "En_e13e3f0a"
      ],
      "members": [
        {
          "id": "En_b71727ac",
          "entity_name": "opaque AI systems",
          "entity_description": "AI systems whose internal logic and decision processes are not easily observable or understandable, making their behavior difficult to interpret.",
          "entity_type_hint": "SystemCharacteristic"
        }
      ],
      "remarks": []
    }
  ],
  "llm_raw": "[\n  {\n    \"function\": \"reassign_entities\",\n    \"args\": {\n      \"entity_ids\": [\"En_e13e3f0a\"],\n      \"from_class_id\": \"ClsC_75828e8c\",\n      \"to_class_id\": \"ClsC_2437d804\",\n      \"justification\": \"The entity 'lack of transparency' describes the condition where AI systems’ decision-making is not visible or understandable, which is conceptually aligned with the 'Opaque AI Systems' class focused on non-interpretable internal logic, rather than with the positive property of transparency.\",\n      \"remark\": null,\n      \"confidence\": 0.86\n    }\n  },\n  {\n    \"function\": \"modify_class\",\n    \"args\": {\n      \"class_id\": \"ClsC_75828e8c\",\n      \"new_name\": \"AI System Transparency\",\n      \"new_description\": \"The degree to which AI systems’ processes and decision logic are open, understandable, and explainable to stakeholders, treated as a positive property and design practice that enhances trust, accountability, and acceptance.\",\n      \"new_class_type_hint\": \"Socio-Technical Concept\",\n      \"new_class_group\": \"AI Transparency & Explainability\",\n      \"remark\": null,\n      \"justification\": \"After moving the 'lack of transparency' condition to the opacity-focused class, this class should clearly represent the positive property and practice of transparency and be grouped within a shared AI transparency/explainability domain.\",\n      \"confidence\": 0.9\n    }\n  },\n  {\n    \"function\": \"modify_class\",\n    \"args\": {\n      \"class_id\": \"ClsC_2437d804\",\n      \"new_name\": \"Opaque and Non-Transparent AI Systems\",\n      \"new_description\": \"AI systems whose internal decision-making processes are not readily interpretable, observable, or transparent to stakeholders, creating a condition of opacity that challenges oversight, accountability, and trust.\",\n      \"new_class_type_hint\": \"System Characteristic\",\n      \"new_class_group\": \"AI Transparency & Explainability\",\n      \"remark\": null,\n      \"justification\": \"Incorporating the 'lack of transparency' entity broadens this class slightly from pure opacity to encompass non-transparent conditions; the updated name/description clarify scope and align it with the shared AI transparency/explainability group.\",\n      \"confidence\": 0.9\n    }\n  },\n  {\n    \"function\": \"modify_class\",\n    \"args\": {\n      \"class_id\": \"ClsC_890d54c4\",\n      \"new_name\": \"AI Decision Explainability\",\n      \"new_description\": \"Concepts related to AI systems’ decisions and the human-understandable explanations of how those decisions are produced, supporting transparency, accountability, and stakeholder understanding.\",\n      \"new_class_type_hint\": \"AI Governance Concept\",\n      \"new_class_group\": \"AI Transparency & Explainability\",\n      \"remark\": null,\n      \"justification\": \"This class focuses on explanations of AI decisions, which is tightly related to but distinct from general transparency and opacity; assigning it to the 'AI Transparency & Explainability' group connects it coherently with the other two classes.\",\n      \"confidence\": 0.9\n    }\n  }\n]",
  "parsed_steps": [
    {
      "function": "reassign_entities",
      "args": {
        "entity_ids": [
          "En_e13e3f0a"
        ],
        "from_class_id": "ClsC_75828e8c",
        "to_class_id": "ClsC_2437d804",
        "justification": "The entity 'lack of transparency' describes the condition where AI systems' decision-making is not visible or understandable, which is conceptually aligned with the 'Opaque AI Systems' class focused on non-interpretable internal logic, rather than with the positive property of transparency.",
        "remark": null,
        "confidence": 0.86
      }
    },
    {
      "function": "modify_class",
      "args": {
        "class_id": "ClsC_75828e8c",
        "new_name": "AI System Transparency",
        "new_description": "The degree to which AI systems' processes and decision logic are open, understandable, and explainable to stakeholders, treated as a positive property and design practice that enhances trust, accountability, and acceptance.",
        "new_class_type_hint": "Socio-Technical Concept",
        "new_class_group": "AI Transparency & Explainability",
        "remark": null,
        "justification": "After moving the 'lack of transparency' condition to the opacity-focused class, this class should clearly represent the positive property and practice of transparency and be grouped within a shared AI transparency/explainability domain.",
        "confidence": 0.9
      }
    },
    {
      "function": "modify_class",
      "args": {
        "class_id": "ClsC_2437d804",
        "new_name": "Opaque and Non-Transparent AI Systems",
        "new_description": "AI systems whose internal decision-making processes are not readily interpretable, observable, or transparent to stakeholders, creating a condition of opacity that challenges oversight, accountability, and trust.",
        "new_class_type_hint": "System Characteristic",
        "new_class_group": "AI Transparency & Explainability",
        "remark": null,
        "justification": "Incorporating the 'lack of transparency' entity broadens this class slightly from pure opacity to encompass non-transparent conditions; the updated name/description clarify scope and align it with the shared AI transparency/explainability group.",
        "confidence": 0.9
      }
    },
    {
      "function": "modify_class",
      "args": {
        "class_id": "ClsC_890d54c4",
        "new_name": "AI Decision Explainability",
        "new_description": "Concepts related to AI systems' decisions and the human-understandable explanations of how those decisions are produced, supporting transparency, accountability, and stakeholder understanding.",
        "new_class_type_hint": "AI Governance Concept",
        "new_class_group": "AI Transparency & Explainability",
        "remark": null,
        "justification": "This class focuses on explanations of AI decisions, which is tightly related to but distinct from general transparency and opacity; assigning it to the 'AI Transparency & Explainability' group connects it coherently with the other two classes.",
        "confidence": 0.9
      }
    }
  ],
  "executed_decisions": [
    {
      "action": "reassign_entities",
      "entity_ids": [
        "En_e13e3f0a"
      ],
      "from": "ClsC_75828e8c",
      "from_resolved": "ClsC_75828e8c",
      "to": "ClsC_2437d804",
      "to_resolved": "ClsC_2437d804",
      "justification": "The entity 'lack of transparency' describes the condition where AI systems' decision-making is not visible or understandable, which is conceptually aligned with the 'Opaque AI Systems' class focused on non-interpretable internal logic, rather than with the positive property of transparency.",
      "remark": null,
      "confidence": 0.86
    },
    {
      "action": "modify_class",
      "class_id": "ClsC_75828e8c",
      "class_id_resolved": "ClsC_75828e8c",
      "new_name": "AI System Transparency",
      "new_description": "The degree to which AI systems' processes and decision logic are open, understandable, and explainable to stakeholders, treated as a positive property and design practice that enhances trust, accountability, and acceptance.",
      "new_class_type_hint": "Socio-Technical Concept",
      "new_class_group": "AI Transparency & Explainability",
      "remark": null,
      "justification": "After moving the 'lack of transparency' condition to the opacity-focused class, this class should clearly represent the positive property and practice of transparency and be grouped within a shared AI transparency/explainability domain.",
      "confidence": 0.9
    },
    {
      "action": "modify_class",
      "class_id": "ClsC_2437d804",
      "class_id_resolved": "ClsC_2437d804",
      "new_name": "Opaque and Non-Transparent AI Systems",
      "new_description": "AI systems whose internal decision-making processes are not readily interpretable, observable, or transparent to stakeholders, creating a condition of opacity that challenges oversight, accountability, and trust.",
      "new_class_type_hint": "System Characteristic",
      "new_class_group": "AI Transparency & Explainability",
      "remark": null,
      "justification": "Incorporating the 'lack of transparency' entity broadens this class slightly from pure opacity to encompass non-transparent conditions; the updated name/description clarify scope and align it with the shared AI transparency/explainability group.",
      "confidence": 0.9
    },
    {
      "action": "modify_class",
      "class_id": "ClsC_890d54c4",
      "class_id_resolved": "ClsC_890d54c4",
      "new_name": "AI Decision Explainability",
      "new_description": "Concepts related to AI systems' decisions and the human-understandable explanations of how those decisions are produced, supporting transparency, accountability, and stakeholder understanding.",
      "new_class_type_hint": "AI Governance Concept",
      "new_class_group": "AI Transparency & Explainability",
      "remark": null,
      "justification": "This class focuses on explanations of AI decisions, which is tightly related to but distinct from general transparency and opacity; assigning it to the 'AI Transparency & Explainability' group connects it coherently with the other two classes.",
      "confidence": 0.9
    }
  ],
  "provisional_to_real": {},
  "timestamp": "2026-01-02T08:43:42Z"
}